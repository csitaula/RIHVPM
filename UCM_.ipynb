{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-15 11:55:44.941780: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "# hrv2, hrv_4, input_tensor with cbam added to see how it works on nwpu\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Reshape, Dense, multiply, add, Permute, Conv2D, Add, \\\n",
    "    Concatenate, Multiply, LSTM, Flatten, Activation, SeparableConv2D, average\n",
    "import numpy as np\n",
    "# import tensorflow.keras.layers\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, Bidirectional, Conv2D, add, TimeDistributed, GlobalAveragePooling2D, \\\n",
    "    Dropout, \\\n",
    "    Concatenate, concatenate, \\\n",
    "    Dense, GlobalMaxPooling2D, MaxPooling2D, Lambda, Add, Flatten, BatchNormalization, Activation\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler, EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.applications.densenet import DenseNet201, DenseNet121, DenseNet169\n",
    "import math\n",
    "from tensorflow.keras.layers import Input, Bidirectional, Conv2D, add, TimeDistributed, GlobalAveragePooling2D, \\\n",
    "    Dropout, \\\n",
    "    Concatenate, concatenate, \\\n",
    "    Dense, GlobalMaxPooling2D, MaxPooling2D, Lambda, Add, Layer, BatchNormalization, ReLU, AveragePooling2D, UpSampling2D\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, GlobalMaxPooling2D, Reshape, Dense, multiply, Multiply, \\\n",
    "    Permute, Concatenate, \\\n",
    "    Conv2D, Add, Activation, Lambda, add, BatchNormalization\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Reshape, Dense, multiply, add, Permute, Conv2D, Add, \\\n",
    "    Concatenate, Multiply, LSTM, Flatten, Activation, SeparableConv2D, average\n",
    "import random\n",
    "import keras_preprocessing.image\n",
    "#!pip install tensorflow==2.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default GPU Device:/device:GPU:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-15 11:55:47.727138: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /device:GPU:0 with 78409 MB memory:  -> device: 0, name: NVIDIA A100 80GB PCIe, pci bus id: 0000:ca:00.0, compute capability: 8.0\n",
      "2023-12-15 11:55:47.733088: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /device:GPU:0 with 78409 MB memory:  -> device: 0, name: NVIDIA A100 80GB PCIe, pci bus id: 0000:ca:00.0, compute capability: 8.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "\n",
    "if tf.test.gpu_device_name():\n",
    "    print('Default GPU Device:{}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"Please install GPU version of TF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.11.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install --no-cache-dir -I tensorflow==2.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ConvBlock(Layer):\n",
    "\n",
    "    def __init__(self, filters=256, kernel_size=3, dilation_rate=1, **kwargs):\n",
    "        super(ConvBlock, self).__init__(**kwargs)\n",
    "\n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.dilation_rate = dilation_rate\n",
    "\n",
    "        self.net = Sequential([\n",
    "            Conv2D(filters, kernel_size=kernel_size, padding='same', dilation_rate=dilation_rate, use_bias=False,\n",
    "                   kernel_initializer='he_normal'),\n",
    "            BatchNormalization(),\n",
    "            ReLU()\n",
    "        ])\n",
    "\n",
    "    def call(self, X):\n",
    "        return self.net(X)\n",
    "\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {\n",
    "            **base_config,\n",
    "            \"filters\": self.filters,\n",
    "            \"kernel_size\": self.kernel_size,\n",
    "            \"dilation_rate\": self.dilation_rate,\n",
    "        }\n",
    "\n",
    "\n",
    "def AtrousSpatialPyramidPooling(X):\n",
    "    B, H, W, C = X.shape\n",
    "\n",
    "    # Image Pooling\n",
    "    image_pool = AveragePooling2D(pool_size=(H, W))(X)\n",
    "    image_pool = ConvBlock(kernel_size=1)(image_pool)\n",
    "    image_pool = UpSampling2D(size=(H // image_pool.shape[1], W // image_pool.shape[2]),\n",
    "                              )(image_pool)\n",
    "\n",
    "    # Atrous Operaions using dilation\n",
    "    conv_1 = ConvBlock(kernel_size=1, dilation_rate=1)(X)\n",
    "    conv_6 = ConvBlock(kernel_size=3, dilation_rate=6)(X)\n",
    "    conv_12 = ConvBlock(kernel_size=3, dilation_rate=12)(X)\n",
    "    conv_18 = ConvBlock(kernel_size=3, dilation_rate=18)(X)\n",
    "\n",
    "    # Combine All\n",
    "    combined = Concatenate()([image_pool, conv_1, conv_6, conv_12, conv_18])\n",
    "    processed = ConvBlock(kernel_size=1)(combined)\n",
    "\n",
    "    # Final Output\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cbam_block(cbam_feature, ratio=8):\n",
    "    \"\"\"Contains the implementation of Convolutional Block Attention Module(CBAM) block.\n",
    "    As described in https://arxiv.org/abs/1807.06521.\n",
    "    \"\"\"\n",
    "\n",
    "    cbam_feature = channel_attention(cbam_feature, ratio)\n",
    "    cbam_feature = spatial_attention(cbam_feature)\n",
    "\n",
    "    return cbam_feature\n",
    "\n",
    "\n",
    "# improved cbam block for remote sensing\n",
    "\n",
    "def cbam_block_improved(cbam_feature, ratio=8):\n",
    "    cbam_channel = channel_attention(cbam_feature, ratio)\n",
    "    cbam_spatial = spatial_attention(cbam_feature)\n",
    "    result1 = multiply([cbam_channel, cbam_spatial])  # was multiply previously and performance was around 92.4\n",
    "    result2 = cbam_block(cbam_feature, ratio)\n",
    "    # combine both types of information\n",
    "    cbam_feature = Add()([result1, result2])  # previously it was  Add()\n",
    "    return cbam_feature\n",
    "\n",
    "\n",
    "# new attention improved\n",
    "def cbam_block_improved_new(cbam_feature, ratio=8):\n",
    "    cbam_channel = channel_attention(cbam_feature, ratio)\n",
    "    cbam_spatial = spatial_attention(cbam_feature)\n",
    "    result1 = multiply([cbam_channel, cbam_spatial])\n",
    "    result2 = cbam_block(cbam_feature, ratio)\n",
    "    # combine both types of information\n",
    "    cbam_feature = Concatenate()([result1, result2,cbam_channel, cbam_spatial,cbam_feature])  # previously it was  Add()\n",
    "    return cbam_feature\n",
    "\n",
    "\n",
    "def channel_attention(input_feature, ratio=8):\n",
    "    input_feature = Conv2D(256, (1, 1), activation='relu')(input_feature)\n",
    "    channel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
    "    channel = input_feature.shape[channel_axis]\n",
    "\n",
    "    shared_layer_one = Dense(channel // ratio,\n",
    "                             activation='relu',\n",
    "                             kernel_initializer='he_normal',\n",
    "                             use_bias=True,\n",
    "                             bias_initializer='zeros')\n",
    "    shared_layer_two = Dense(channel,\n",
    "                             kernel_initializer='he_normal',\n",
    "                             use_bias=True,\n",
    "                             bias_initializer='zeros')\n",
    "\n",
    "    avg_pool = GlobalAveragePooling2D()(input_feature)\n",
    "    avg_pool = Reshape((1, 1, channel))(avg_pool)\n",
    "    assert avg_pool.shape[1:] == (1, 1, channel)\n",
    "    avg_pool = shared_layer_one(avg_pool)\n",
    "    assert avg_pool.shape[1:] == (1, 1, channel // ratio)\n",
    "    avg_pool = shared_layer_two(avg_pool)\n",
    "    assert avg_pool.shape[1:] == (1, 1, channel)\n",
    "\n",
    "    max_pool = GlobalMaxPooling2D()(input_feature)\n",
    "    max_pool = Reshape((1, 1, channel))(max_pool)\n",
    "    assert max_pool.shape[1:] == (1, 1, channel)\n",
    "    max_pool = shared_layer_one(max_pool)\n",
    "    assert max_pool.shape[1:] == (1, 1, channel // ratio)\n",
    "    max_pool = shared_layer_two(max_pool)\n",
    "    assert max_pool.shape[1:] == (1, 1, channel)\n",
    "\n",
    "    cbam_feature = Add()([avg_pool, max_pool])\n",
    "    cbam_feature = Activation('sigmoid')(cbam_feature)\n",
    "\n",
    "    if K.image_data_format() == \"channels_first\":\n",
    "        cbam_feature = Permute((3, 1, 2))(cbam_feature)\n",
    "    # print(cbam_feature.shape)\n",
    "    # print(input_feature.shape)\n",
    "    # element wise application\n",
    "    return multiply([input_feature, cbam_feature])\n",
    "\n",
    "\n",
    "def spatial_attention(input_feature):\n",
    "    input_feature = Conv2D(256, (1, 1), activation='relu')(input_feature)\n",
    "    kernel_size = 7\n",
    "    if K.image_data_format() == \"channels_first\":\n",
    "        channel = input_feature.shape[1]\n",
    "        cbam_feature = Permute((2, 3, 1))(input_feature)\n",
    "    else:\n",
    "        channel = input_feature.shape[-1]\n",
    "        cbam_feature = input_feature\n",
    "\n",
    "    avg_pool = Lambda(lambda x: K.mean(x, axis=3, keepdims=True))(cbam_feature)\n",
    "    assert avg_pool.shape[-1] == 1\n",
    "    max_pool = Lambda(lambda x: K.max(x, axis=3, keepdims=True))(cbam_feature)\n",
    "    assert max_pool.shape[-1] == 1\n",
    "    concat = Concatenate(axis=3)([avg_pool, max_pool])\n",
    "    assert concat.shape[-1] == 2\n",
    "    cbam_feature = Conv2D(filters=1,\n",
    "                          kernel_size=kernel_size,\n",
    "                          strides=1,\n",
    "                          padding='same',\n",
    "                          activation='sigmoid',\n",
    "                          kernel_initializer='he_normal',\n",
    "                          use_bias=False)(concat)\n",
    "    assert cbam_feature.shape[-1] == 1\n",
    "\n",
    "    if K.image_data_format() == \"channels_first\":\n",
    "        cbam_feature = Permute((3, 1, 2))(cbam_feature)\n",
    "\n",
    "    return multiply([input_feature, cbam_feature])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_crop_img(path, grayscale=False, color_mode='rgb', target_size=None,\n",
    "                      interpolation='nearest'):\n",
    "    \"\"\"Wraps keras_preprocessing.image.utils.loag_img() and adds cropping.\n",
    "    Cropping method enumarated in interpolation\n",
    "    # Arguments\n",
    "        path: Path to image file.\n",
    "        color_mode: One of \"grayscale\", \"rgb\", \"rgba\". Default: \"rgb\".\n",
    "            The desired image format.\n",
    "        target_size: Either `None` (default to original size)\n",
    "            or tuple of ints `(img_height, img_width)`.\n",
    "        interpolation: Interpolation and crop methods used to resample and crop the image\n",
    "            if the target size is different from that of the loaded image.\n",
    "            Methods are delimited by \":\" where first part is interpolation and second is crop\n",
    "            e.g. \"lanczos:random\".\n",
    "            Supported interpolation methods are \"nearest\", \"bilinear\", \"bicubic\", \"lanczos\",\n",
    "            \"box\", \"hamming\" By default, \"nearest\" is used.\n",
    "            Supported crop methods are \"none\", \"center\", \"random\".\n",
    "    # Returns\n",
    "        A PIL Image instance.\n",
    "    # Raises\n",
    "        ImportError: if PIL is not available.\n",
    "        ValueError: if interpolation method is not supported.\n",
    "    \"\"\"\n",
    "\n",
    "    # Decode interpolation string. Allowed Crop methods: none, center, random\n",
    "    interpolation, crop = interpolation.split(\":\") if \":\" in interpolation else (interpolation, \"none\")\n",
    "\n",
    "    if crop == \"none\":\n",
    "        return keras_preprocessing.image.utils.load_img(path,\n",
    "                                                        grayscale=grayscale,\n",
    "                                                        color_mode=color_mode,\n",
    "                                                        target_size=target_size,\n",
    "                                                        interpolation=interpolation)\n",
    "\n",
    "    # Load original size image using Keras\n",
    "    img = keras_preprocessing.image.utils.load_img(path,\n",
    "                                                   grayscale=grayscale,\n",
    "                                                   color_mode=color_mode,\n",
    "                                                   target_size=None,\n",
    "                                                   interpolation=interpolation)\n",
    "\n",
    "    # Crop fraction of total image\n",
    "    crop_fraction = 0.875\n",
    "    target_width = target_size[1]\n",
    "    target_height = target_size[0]\n",
    "\n",
    "    if target_size is not None:\n",
    "        if img.size != (target_width, target_height):\n",
    "\n",
    "            if crop not in [\"center\", \"random\"]:\n",
    "                raise ValueError('Invalid crop method {} specified.', crop)\n",
    "\n",
    "            if interpolation not in keras_preprocessing.image.utils._PIL_INTERPOLATION_METHODS:\n",
    "                raise ValueError(\n",
    "                    'Invalid interpolation method {} specified. Supported '\n",
    "                    'methods are {}'.format(interpolation,\n",
    "                                            \", \".join(\n",
    "                                                keras_preprocessing.image.utils._PIL_INTERPOLATION_METHODS.keys())))\n",
    "\n",
    "            resample = keras_preprocessing.image.utils._PIL_INTERPOLATION_METHODS[interpolation]\n",
    "\n",
    "            width, height = img.size\n",
    "\n",
    "            # Resize keeping aspect ratio\n",
    "            # result shold be no smaller than the targer size, include crop fraction overhead\n",
    "            target_size_before_crop = (target_width / crop_fraction, target_height / crop_fraction)\n",
    "            ratio = max(target_size_before_crop[0] / width, target_size_before_crop[1] / height)\n",
    "            target_size_before_crop_keep_ratio = int(width * ratio), int(height * ratio)\n",
    "            img = img.resize(target_size_before_crop_keep_ratio, resample=resample)\n",
    "\n",
    "            width, height = img.size\n",
    "\n",
    "            if crop == \"center\":\n",
    "                left_corner = int(round(width / 2)) - int(round(target_width / 2))\n",
    "                top_corner = int(round(height / 2)) - int(round(target_height / 2))\n",
    "                return img.crop((left_corner, top_corner, left_corner + target_width, top_corner + target_height))\n",
    "            elif crop == \"random\":\n",
    "                left_shift = random.randint(0, int((width - target_width)))\n",
    "                down_shift = random.randint(0, int((height - target_height)))\n",
    "                return img.crop((left_shift, down_shift, target_width + left_shift, target_height + down_shift))\n",
    "\n",
    "    return img\n",
    "\n",
    "# Monkey patch\n",
    "keras_preprocessing.image.iterator.load_img = load_and_crop_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squeeze_excite_block(input, ratio=16):\n",
    "    ''' Create a channel-wise squeeze-excite block\n",
    "    Args:\n",
    "        input: input tensor\n",
    "        filters: number of output filters\n",
    "    Returns: a keras tensor\n",
    "    References\n",
    "    -   [Squeeze and Excitation Networks](https://arxiv.org/abs/1709.01507)\n",
    "    '''\n",
    "    init = input\n",
    "    channel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
    "    filters = init.shape[channel_axis]\n",
    "    se_shape = (1, 1, filters)\n",
    "\n",
    "    se = GlobalAveragePooling2D()(init)\n",
    "    se = Reshape(se_shape)(se)\n",
    "    se = Dense(filters // ratio, activation='relu', kernel_initializer='he_normal', use_bias=False)(se)\n",
    "    se = Dense(filters, activation='sigmoid', kernel_initializer='he_normal', use_bias=False)(se)\n",
    "\n",
    "    # if K.image_data_format() == 'channels_first':\n",
    "    #     se = Permute((3, 1, 2))(se)\n",
    "\n",
    "    print(init.shape)\n",
    "    print(se.shape)\n",
    "    x = multiply([init, se])\n",
    "    return x\n",
    "\n",
    "\n",
    "def spatial_squeeze_excite_block(input):\n",
    "    ''' Create a spatial squeeze-excite block\n",
    "    Args:\n",
    "        input: input tensor\n",
    "    Returns: a keras tensor\n",
    "    References\n",
    "    -   [Concurrent Spatial and Channel Squeeze & Excitation in Fully Convolutional Networks](https://arxiv.org/abs/1803.02579)\n",
    "    '''\n",
    "\n",
    "    se = Conv2D(1, (1, 1), activation='sigmoid', use_bias=False,\n",
    "                kernel_initializer='he_normal')(input)\n",
    "\n",
    "    x = multiply([input, se])\n",
    "    return x\n",
    "\n",
    "\n",
    "# Adaptive ECA module\n",
    "def eca_module(inputs, gamma=2, b=1):\n",
    "    x = inputs\n",
    "#     t = int(abs((K.int_shape(x)[3] * gamma) // b))\n",
    "#     k = t if t % 2 else t + 1\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Reshape((1, 1, x.shape[1]))(x)\n",
    "    x = Conv2D(1, (3, 3), padding='same', kernel_initializer='he_normal', use_bias=False)(x) # 3 by 3 for k\n",
    "    x = Activation('sigmoid')(x)\n",
    "    x = multiply([inputs, x])\n",
    "    return x\n",
    "\n",
    "def BAM(inputs,reduction_ratio=16, dilation_value=4, reuse=None, scope='BAM'):\n",
    "#     with tf.variable_scope(scope, reuse=reuse):\n",
    "#         with tf.compat.v1.keras.backend.name_scope(scope):\n",
    "#             with tf.compat.v1.keras.backend.name_scope('BAM'):\n",
    "    input_channel = inputs.get_shape().as_list()[-1]\n",
    "    num_squeeze = input_channel // reduction_ratio\n",
    "\n",
    "    # Channel attention\n",
    "    gap = tf.reduce_mean(inputs, axis=[1, 2], keepdims=True)\n",
    "    channel = Dense(num_squeeze, activation=None, kernel_initializer='glorot_uniform',\n",
    "                           kernel_regularizer=tf.keras.regularizers.l2(0.0005))(gap)\n",
    "    channel = Dense(input_channel, activation=None, kernel_initializer='glorot_uniform',\n",
    "                           kernel_regularizer=tf.keras.regularizers.l2(0.0005),\n",
    "                           )(channel)\n",
    "    channel = BatchNormalization()(channel)\n",
    "\n",
    "    # Spatial attention\n",
    "    spatial = Conv2D(num_squeeze, kernel_size=1, padding='same', activation=None,\n",
    "                            )(inputs)\n",
    "    spatial = Conv2D(num_squeeze, kernel_size=3, padding='same', activation=None,\n",
    "                            dilation_rate=dilation_value)(spatial)\n",
    "    spatial = Conv2D(num_squeeze, kernel_size=3, padding='same', activation=None,\n",
    "                            dilation_rate=dilation_value)(spatial)\n",
    "    spatial = Conv2D(1, kernel_size=1, padding='same', activation=None,\n",
    "                            )(spatial)\n",
    "    spatial = BatchNormalization()(spatial)\n",
    "\n",
    "    # Combined two attention branches\n",
    "    combined = tf.sigmoid(channel + spatial)\n",
    "\n",
    "    output = inputs + inputs * combined\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def channel_spatial_squeeze_excite(input, ratio=16):\n",
    "    ''' Create a spatial squeeze-excite block\n",
    "    Args:\n",
    "        input: input tensor\n",
    "        filters: number of output filters\n",
    "    Returns: a keras tensor\n",
    "    References\n",
    "    -   [Squeeze and Excitation Networks](https://arxiv.org/abs/1709.01507)\n",
    "    -   [Concurrent Spatial and Channel Squeeze & Excitation in Fully Convolutional Networks](https://arxiv.org/abs/1803.02579)\n",
    "    '''\n",
    "\n",
    "    cse = squeeze_excite_block(input, ratio)\n",
    "    sse = spatial_squeeze_excite_block(input)\n",
    "\n",
    "    x = add([cse, sse])\n",
    "    return x\n",
    "\n",
    "\n",
    "# Change the number of filters to 1280 for more information or keep 2688 as it is with 1 by 1 convolution,previously 1024\n",
    "def conv1by1(input_tensor):\n",
    "    tensor = Conv2D(512, (1, 1), activation='relu')(input_tensor)  # 256-D provides around 94% acc.\n",
    "    # tensor = squeeze_excite_block(tensor)\n",
    "    return tensor\n",
    "\n",
    "\n",
    "def sequence_layer(input):\n",
    "    print(input.shape)\n",
    "    # input= conv1by1(input)\n",
    "    units = input.shape[1] * input.shape[2]\n",
    "    reshape = Reshape((units, input.shape[3]))(input)\n",
    "    lstm_layer = LSTM(49, input_shape=(units, input.shape[3]), return_sequences=True)(reshape)\n",
    "    flatten = Flatten()(lstm_layer)\n",
    "    return flatten\n",
    "\n",
    "def EARCM_improved_TGRS(input_tensor):\n",
    "    units=512\n",
    "    lev_1 = Conv2D(units, (1, 1), padding='same')(input_tensor)  # 1x1 convolution and reduce channel\n",
    "    lev_3 = Conv2D(units, (3, 3), padding='same')(input_tensor)  # 3x3 convolution and reduce channel\n",
    "    lev_5 = Conv2D(units, (5, 5), padding='same')(input_tensor)  # 5x5 convolution and reduce channel\n",
    "\n",
    "    # CBAM modified block\n",
    "    cbam_imp = cbam_block_improved_new(input_tensor) #original cbam (cbam_block) worked to produce over 94.60% accuracy\n",
    "    # proposed approach is concat\n",
    "    res1 = Concatenate()(\n",
    "        [lev_1, lev_3, lev_5, cbam_imp])  # prev, [lev_1, lev_3, lev_5]===94.40%, added input_tensor\n",
    "\n",
    "    # convolution 1 by 1 for the fusion\n",
    "    #res1= conv1by1(res1) #this step was not taken previously\n",
    "    # # eca for the selection of interesting regions after fusion\n",
    "    eca = eca_module(res1)  # eca worked well! acc: 94.40%\n",
    "    #eca=channel_spatial_squeeze_excite(conv) # testing this although eca worked fine to some extent\n",
    "    # bn=BatchNormalization()(eca)\n",
    "    return eca\n",
    "\n",
    "\n",
    "def multi_scale_msafeb(input_tensor, ratio=4):\n",
    "#     print(input_tensor.shape)\n",
    "    filter = input_tensor.shape[3]\n",
    "    # p0=Modules.AtrousSpatialPyramidPooling(input_tensor), dilation=4, and groups=10, provided 93.80%\n",
    "    pool1 = Conv2D(filters=filter/ratio, kernel_size=1, activation='relu', padding='same', dilation_rate=4,\n",
    "                   groups=8)(\n",
    "        input_tensor)  # previously 12 groups, could try 16 as well for more groups\n",
    "    \n",
    "    pool1_ = GlobalAveragePooling2D()(pool1)\n",
    "    p1 = AtrousSpatialPyramidPooling(pool1)\n",
    "       \n",
    "    # p1 =eca_module(pool1)\n",
    "\n",
    "    pool2 = Conv2D(filters=filter/ratio, kernel_size=3, activation='relu', padding='same', dilation_rate=4,\n",
    "                   groups=8)(input_tensor)\n",
    "    \n",
    "    pool21= Conv2D(filters=filter/ratio, kernel_size=3, activation='relu', padding='same', dilation_rate=4,\n",
    "                   groups=8)(pool1)\n",
    "    \n",
    "    pool2_ = GlobalAveragePooling2D()(pool2)\n",
    "    pool21_=GlobalAveragePooling2D()(pool21)\n",
    "    p2 = AtrousSpatialPyramidPooling(pool2)\n",
    "    p21=AtrousSpatialPyramidPooling(pool21)\n",
    "    # p2=eca_module(pool2)\n",
    "\n",
    "    pool3 = Conv2D(filters=filter/ratio, kernel_size=5, activation='relu', padding='same', dilation_rate=4,\n",
    "                   groups=8)(input_tensor)\n",
    "    pool31 = Conv2D(filters=filter/ratio, kernel_size=5, activation='relu', padding='same', dilation_rate=4,\n",
    "                   groups=8)(pool2)\n",
    "    pool3_ = GlobalAveragePooling2D()(pool3)\n",
    "    pool31_ = GlobalAveragePooling2D()(pool31)\n",
    "    \n",
    "    p3 = AtrousSpatialPyramidPooling(pool3)\n",
    "    p31=AtrousSpatialPyramidPooling(pool31)\n",
    "    # p3=eca_module(pool3)\n",
    "     \n",
    "    conv = concatenate([p1, p2, p3, input_tensor]) #pool1, pool2 and pool3 were working well, testing p1, p2, and p3\n",
    "\n",
    "    #old technique\n",
    "    \n",
    "    conv = conv1by1(conv) # how about changing 256>>512 for conv1by1\n",
    "    \n",
    "    earcm = EARCM_improved_TGRS(conv)  # our own attention block EARCM_improved_TGR() previously\n",
    "    \n",
    "    return earcm, pool1_, pool2_, pool3_,  pool21_,  pool31_\n",
    "\n",
    "\n",
    "def rotation_invariant(input_tensor):\n",
    "    # rotation\n",
    "    o = input_tensor\n",
    "    r1 = tf.image.rot90(input_tensor, k=1) # counterclockwise 90 degree\n",
    "    r2 = tf.image.rot90(input_tensor, k=2) # counterclockwise 180 degree\n",
    "    r3= tf.image.rot90(input_tensor,k=3) # counterclockwise 270 degree\n",
    "    r4= tf.image.rot90(input_tensor,k=-1) # clockwise 90\n",
    "    r5=tf.image.rot90(input_tensor,k=-2) # clockwise 180\n",
    "    r6=tf.image.rot90(input_tensor, k=-3) #clockwise 270\n",
    "\n",
    "\n",
    "    # msafeb\n",
    "    o_m, o_p1, o_p2, o_p3, po1, po2 = multi_scale_msafeb(o)\n",
    "    r1_m, r1_p1, r1_p2, r1_p3, p11, p12 = multi_scale_msafeb(r1)\n",
    "    r2_m, r2_p1, r2_p2, r2_p3, p21, p22 = multi_scale_msafeb(r2)\n",
    "    r3_m, r3_p1, r3_p2, r3_p3, p31, p32 = multi_scale_msafeb(r3)\n",
    "    r4_m, r4_p1, r4_p2, r4_p3, p41, p42 = multi_scale_msafeb(r4)\n",
    "    r5_m, r5_p1, r5_p2, r5_p3, p51, p52 = multi_scale_msafeb(r5)\n",
    "    r6_m, r6_p1, r6_p2, r6_p3, p61, p62 = multi_scale_msafeb(r6)\n",
    "    \n",
    "    \n",
    "    # convert back to original format\n",
    "    r1_m=tf.image.rot90(r1_m,k=-1)\n",
    "    r2_m=tf.image.rot90(r2_m,k=-2)\n",
    "    r3_m=tf.image.rot90(r3_m,k=-3)\n",
    "    r4_m=tf.image.rot90(r4_m,k=1)\n",
    "    r5_m=tf.image.rot90(r5_m,k=2)\n",
    "    r6_m=tf.image.rot90(r6_m,k=3)\n",
    "    \n",
    "    return average([o_m, r1_m,r2_m,r3_m,r4_m,r5_m,r6_m]),average([o_p1, r1_p1,r2_p1,r3_p1,r4_p1,r5_p1,r6_p1]), average([o_p2, r1_p2,r2_p2,r3_p2,r4_p2,r5_p2,r6_p2]), average([o_p3, r1_p3,r2_p3,r3_p3,r4_p3,r5_p3,r6_p3]), average([po1, p11,p21,p31,p41,p51,p61]), average([po2, p12,p22,p32,p42,p52,p62])\n",
    "    \n",
    "\n",
    "def hor_ver_mul(input_tensor,s):\n",
    "    \n",
    "    # rotation\n",
    "    o = input_tensor\n",
    "    r1 = tf.image.rot90(input_tensor, k=1) # counterclockwise 90 degree\n",
    "    r2 = tf.image.rot90(input_tensor, k=2) # counterclockwise 180 degree\n",
    "    r3= tf.image.rot90(input_tensor,k=3) # counterclockwise 270 degree\n",
    "    r4= tf.image.rot90(input_tensor,k=-1) # clockwise 90\n",
    "    r5=tf.image.rot90(input_tensor,k=-2) # clockwise 180\n",
    "    r6=tf.image.rot90(input_tensor, k=-3) #clockwise 270\n",
    "    \n",
    "    #horizontal and vertical multiply\n",
    "    o_m=multi_scale_hrv_pooling(o,s)\n",
    "    r1_m=multi_scale_hrv_pooling(r1,s)\n",
    "    r2_m=multi_scale_hrv_pooling(r2,s)\n",
    "    r3_m=multi_scale_hrv_pooling(r3,s)\n",
    "    r4_m=multi_scale_hrv_pooling(r4,s)\n",
    "    r5_m=multi_scale_hrv_pooling(r5,s)\n",
    "    r6_m=multi_scale_hrv_pooling(r6,s)\n",
    "    \n",
    "    # revert back to the original format\n",
    "    r1_m=tf.image.rot90(r1_m, k=-1)\n",
    "    r2_m=tf.image.rot90(r2_m,k=-2)\n",
    "    r3_m=tf.image.rot90(r3_m,k=-3)\n",
    "    r4_m=tf.image.rot90(r4_m,k=1)\n",
    "    r5_m=tf.image.rot90(r5_m,k=2)\n",
    "    r6_m=tf.image.rot90(r6_m,k=3)\n",
    "    \n",
    "    #avg =average([o_m, r1_m, r2_m, r3_m, r4_m, r5_m, r6_m]) \n",
    "    avg =concatenate([o_m, r1_m, r2_m, r3_m, r4_m, r5_m, r6_m])\n",
    "    #avg= cbam_block_improved_new(avg)\n",
    "    return avg  #average provided over 94.50+\n",
    "\n",
    "\n",
    "# horizontal pooling\n",
    "def hpool(k, x, tsnr):\n",
    "    hp = AveragePooling2D(pool_size=(x, k), padding='same')(tsnr)  \n",
    "    return hp\n",
    "\n",
    "\n",
    "# vertical pooling\n",
    "def vpool(k, x, tsnr):\n",
    "    vp = AveragePooling2D(pool_size=(x, k), padding='same')(tsnr)  \n",
    "    return vp\n",
    "\n",
    "\n",
    "# combination and use attention to capture salient information\n",
    "def hvpool(hp, vp, tnsr):\n",
    "    # matrix multiplication\n",
    "#     print(hp)\n",
    "#     print(vp)\n",
    "    # print(tnsr)\n",
    "    mul = multiply([vp, hp])\n",
    "    # attention layer EARCM: TODO\n",
    "    x = Activation('sigmoid')(mul)\n",
    "#     x = add([tnsr, x])\n",
    "    # mul_ = EACRM(mul)\n",
    "    return x\n",
    "\n",
    "\n",
    "def multi_scale_hrv_pooling(tensor,s):\n",
    "    scales = [1]\n",
    "    tensors = []\n",
    "    # for i in range(0, len(scales)):\n",
    "    vp = vpool(tensor.shape[1], s, tensor)\n",
    "    hp = hpool(s, tensor.shape[2], tensor)\n",
    "    cmb = hvpool(hp, vp, tensor)\n",
    "    # tensors.append(cmb)\n",
    "    return cmb\n",
    "\n",
    "def weighted_pooling(invert2):\n",
    "    alpha = 0.7\n",
    "    a = GlobalAveragePooling2D()(invert2)\n",
    "    a = Lambda(lambda xx: xx * alpha)(a)\n",
    "    m = GlobalMaxPooling2D()(invert2)\n",
    "    m = Lambda(lambda xx: xx * (1 - alpha))(m)\n",
    "    x3 = Add()([a, m])\n",
    "    return x3\n",
    "\n",
    "\n",
    "# https://stackoverflow.com/questions/62877879/implementing-late-fusion-in-keras\n",
    "# Adaptive weighted layer\n",
    "class WeightedAverage(Layer):\n",
    "\n",
    "    def __init__(self, n_output):\n",
    "        super(WeightedAverage, self).__init__()\n",
    "        self.W = tf.Variable(initial_value=tf.random.uniform(shape=[1,1,n_output], minval=0, maxval=1),\n",
    "            trainable=True) # (1,1,n_inputs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "\n",
    "        # inputs is a list of tensor of shape [(n_batch, n_feat), ..., (n_batch, n_feat)]\n",
    "        # expand last dim of each input passed [(n_batch, n_feat, 1), ..., (n_batch, n_feat, 1)]\n",
    "        inputs = [tf.expand_dims(i, -1) for i in inputs]\n",
    "        inputs = Concatenate(axis=-1)(inputs) # (n_batch, n_feat, n_inputs)\n",
    "        weights = tf.nn.softmax(self.W) # (1,1,n_inputs)\n",
    "        # weights sum up to one on last dim\n",
    "        return tf.reduce_sum(weights*inputs, axis=-1) # (n_batch, n_feat)\n",
    "\n",
    "    # # x = [x1,x2] two fc layers\n",
    "# W_Avg = WeightedAverage(n_output=len(x))(x)\n",
    "\n",
    "def weighted_pooling_adaptive(invert2):\n",
    "#     alpha = 0.7\n",
    "    a = GlobalAveragePooling2D()(invert2)\n",
    "#     a = Lambda(lambda xx: xx * alpha)(a)\n",
    "    m = GlobalMaxPooling2D()(invert2)\n",
    "    x = [a,m]\n",
    "    w_avg=WeightedAverage(n_output=len(x))(x)\n",
    "#     m = Lambda(lambda xx: xx * (1 - alpha))(m)\n",
    "#     x3 = Add()([a, m])\n",
    "    return w_avg\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# single level\n",
    "def multi_scale_aspp_hrv(model, classes):\n",
    "    m_ = model.output\n",
    "#     seq=sequence_layer(m_)\n",
    "    \n",
    "    hrv_1=hor_ver_mul(m_,1)\n",
    "#     hrv_1=AtrousSpatialPyramidPooling(hrv_1)\n",
    "    hrv_1=GlobalAveragePooling2D()(hrv_1)\n",
    "      \n",
    "      # hor, ver pooling\n",
    "    hrv_2=hor_ver_mul(m_,2)# ASPP->GAP?\n",
    "    hrv_2=GlobalAveragePooling2D()(hrv_2)\n",
    "    \n",
    "    hrv_3=hor_ver_mul(m_,3) # ASPP->GAP?\n",
    "    hrv_3=GlobalAveragePooling2D()(hrv_3)\n",
    "    \n",
    "    hrv_4=hor_ver_mul(m_,4) # ASPP->GAP?\n",
    "    hrv_4=GlobalAveragePooling2D()(hrv_4)\n",
    "    \n",
    "    hrv_5=hor_ver_mul(m_,5) # ASPP->GAP?\n",
    "    hrv_5=GlobalAveragePooling2D()(hrv_5)\n",
    "    \n",
    "    hrv_6=hor_ver_mul(m_,6) # ASPP->GAP?\n",
    "    hrv_6=GlobalAveragePooling2D()(hrv_6)        \n",
    "#     m, p1_, p2_, p3_,p1, p2 = rotation_invariant(m_)   \n",
    "    #mgap = GlobalAveragePooling2D()(m)    \n",
    "    #o_m, o_p1, o_p2, o_p3, po1, po2 = multi_scale_msafeb(m_)\n",
    "#     m_= conv1by1(m_)\n",
    "#     o_m = GlobalAveragePooling2D()(m_)    \n",
    "#     rot=average([p1_, p2_, p3_]) #initially, concat, now it is average   \n",
    "    #o_m, o_p1, o_p2, o_p3, po1, po2, po3 = multi_scale_msafeb(m_)\n",
    "    avg_hrv= average([hrv_1, hrv_2, hrv_3,hrv_4, hrv_5,hrv_6])\n",
    "    hrv= concatenate([hrv_1, hrv_2, hrv_3,hrv_4, hrv_5,hrv_6,avg_hrv]) #initially concat, not average (1, 3, 5 initially)                \n",
    "#     c = concatenate([rot,hrv]) ### 1, 3, 5, 6 are improving the performance.   \n",
    "    c = Dropout(0.2)(hrv)\n",
    "        \n",
    "    outputs = Dense(classes, activation='softmax')(c)\n",
    "    prop = Model(model.input, outputs)\n",
    "    return prop\n",
    "   \n",
    "# learning decay rate schedule\n",
    "def step_decay(epoch):\n",
    "    initial_lrate = 0.0001\n",
    "    drop = 0.4  # in 0.5 it provided an accuracy of 80%+\n",
    "    epochs_drop = 4.0  # 5.0 gives an optimal epochs_drop\n",
    "    lrate = initial_lrate * math.pow(drop, math.floor((1 + epoch) / epochs_drop))\n",
    "    return lrate\n",
    "\n",
    "\n",
    "def train_ml(model, train_batches, valid_batches, classes):\n",
    "    m = custom_resnet50(model, classes)\n",
    "    #m =multi_scale_aspp_hrv(model, classes)\n",
    "    # m = fine_tune(model, classes)\n",
    "    # m=ensemble(classes)\n",
    "    #m=model\n",
    "    # print(m.summary())\n",
    "    # m = model\n",
    "    NUM_EPOCHS = 50\n",
    "    LEARNING_RATE = 0.0003  # for one step\n",
    "    m.compile(loss='categorical_crossentropy',  # for multiclass use categorical_crossentropy\n",
    "              # optimizer=optimizers.SGD(lr=LEARNING_RATE,momentum=0.9),\n",
    "              optimizer=optimizers.Adam(lr=LEARNING_RATE),\n",
    "              #  optimizer=optimizers.Adam(lr_schedule),\n",
    "              metrics=['acc'])\n",
    "\n",
    "    # print(model.summary())\n",
    "    # learning schedule callback\n",
    "    # es = EarlyStopping(monitor='val_loss', patience=5)\n",
    "    lrate = LearningRateScheduler(step_decay)\n",
    "    es = EarlyStopping(monitor='val_loss', patience=5)\n",
    "    mcp_save = ModelCheckpoint('/data/gpfs/projects/punim2008/data/Fold' + '_step1.h5', save_best_only=True, monitor='val_loss', mode='min')\n",
    "    callbacks_list = [lrate]\n",
    "\n",
    "    STEP_SIZE_TRAIN = train_batches.n // train_batches.batch_size\n",
    "    STEP_SIZE_VALID = valid_batches.n // valid_batches.batch_size\n",
    "    # lr_decay = LearningRateScheduler(schedule=lambda epoch: LEARNING_RATE * (0.9 ** epoch))\n",
    "    # callbacks_list=[es]\n",
    "    m.fit_generator(train_batches,\n",
    "                    steps_per_epoch=STEP_SIZE_TRAIN,\n",
    "                    validation_data=valid_batches,\n",
    "                    validation_steps=STEP_SIZE_VALID,\n",
    "                    epochs=NUM_EPOCHS,\n",
    "                    callbacks=callbacks_list\n",
    "                    )\n",
    "\n",
    "    x = m.evaluate_generator(valid_batches,\n",
    "                             steps=np.ceil(len(valid_batches)),\n",
    "                             use_multiprocessing=False,\n",
    "                             verbose=1,\n",
    "                             workers=1,\n",
    "                             )\n",
    "    #  print('Testing time:' + str(time.clock() - test_s_time) + 'secs.')\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:1\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-15 11:55:49.302166: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 78409 MB memory:  -> device: 0, name: NVIDIA A100 80GB PCIe, pci bus id: 0000:ca:00.0, compute capability: 8.0\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/data/gpfs/projects/punim2008/data/NWPU_/2_8/1/train'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [10], line 37\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# DATASET_PATH = root_path/train'\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# test_dir = 'root_path/val'\u001b[39;00m\n\u001b[1;32m     36\u001b[0m IMAGE_SIZE \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m)\n\u001b[0;32m---> 37\u001b[0m data_list \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDATASET_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# data_list = os.listdir('D:/COVID/four_classes/splits/f4/train')\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Delete some classes that may interfere\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(data_list))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/data/gpfs/projects/punim2008/data/NWPU_/2_8/1/train'"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for i in range(0, 5):\n",
    "    print('Fold:' + str(i + 1))\n",
    "    print(\"*\" * 100)\n",
    "    # Two steps training process\n",
    "    # Step 1\n",
    "    model = DenseNet201(include_top=False, weights='imagenet', input_shape=(224, 224, 3))\n",
    "\n",
    "    # model = m1\n",
    "    #model = DenseNet201(include_top=False, weights='imagenet', input_shape=(224, 224, 3))\n",
    "    #model.trainable = True\n",
    "    \n",
    "    #model =tf.keras.applications.ConvNeXtTiny( model_name=\"convnext_tiny\",\n",
    "    #include_top=True,\n",
    "    #include_preprocessing=True,\n",
    "    #weights=\"imagenet\",\n",
    "    #input_tensor=None,\n",
    "    #input_shape=None,\n",
    "    #pooling=None,\n",
    "    #classes=1000,\n",
    "    #classifier_activation=\"softmax\")\n",
    "    #model.trainable = True\n",
    "    \n",
    "    # Step 2\n",
    "#     model = load_model('/data/gpfs/projects/punim2008/data/Fold_step1.h5',custom_objects={'ConvBlock':ConvBlock})\n",
    "#     model.trainable = True\n",
    "#     model.summary()\n",
    "    # exit()\n",
    "    # data load and train\n",
    "    # root_path = \"D://Jagannath_dai/AID_/2_8/\" + str(i + 1) + '/'\n",
    "    root_path = \"/data/gpfs/projects/punim2008/data/NWPU_/2_8/\" + str(i + 1) + '/'\n",
    "    DATASET_PATH = root_path + 'train'\n",
    "    test_dir = root_path + 'val'\n",
    "    # DATASET_PATH = root_path/train'\n",
    "    # test_dir = 'root_path/val'\n",
    "    IMAGE_SIZE = (224, 224)\n",
    "    data_list = os.listdir(DATASET_PATH)\n",
    "    # data_list = os.listdir('D:/COVID/four_classes/splits/f4/train')\n",
    "    # Delete some classes that may interfere\n",
    "    print(len(data_list))\n",
    "    NUM_CLASSES = len(data_list)\n",
    "    BATCH_SIZE = 16  # try reducing batch size or freeze more layers if your GPU runs out of memory\n",
    "\n",
    "    # Train datagen here is a preprocessor\n",
    "    train_datagen = ImageDataGenerator(rescale=1. / 255,\n",
    "                                       horizontal_flip=True\n",
    "                                       )\n",
    "\n",
    "    test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "    train_batches = train_datagen.flow_from_directory(DATASET_PATH,\n",
    "                                                      target_size=IMAGE_SIZE,\n",
    "                                                      shuffle=True,\n",
    "                                                      interpolation='lanczos:random',  # <--------- random crop\n",
    "                                                      batch_size=BATCH_SIZE,\n",
    "                                                      # subset=\"training\",\n",
    "                                                      seed=42,\n",
    "                                                      class_mode=\"categorical\"\n",
    "                                                      # For multiclass use categorical n for binary use binary\n",
    "                                                      )\n",
    "\n",
    "    valid_batches = test_datagen.flow_from_directory(test_dir,\n",
    "                                                     target_size=IMAGE_SIZE,\n",
    "                                                     shuffle=True,\n",
    "                                                     batch_size=BATCH_SIZE,\n",
    "                                                     # interpolation = 'lanczos:center', # <--------- center crop\n",
    "                                                     # subset=\"validation\",\n",
    "                                                     seed=42,\n",
    "                                                     class_mode=\"categorical\"\n",
    "                                                     # For multiclass use categorical n for binary use binary\n",
    "                                                     )\n",
    "    x = train_ml(model, train_batches, valid_batches, NUM_CLASSES)\n",
    "    print('Test loss:', x[0])\n",
    "    print('Test accuracy:', x[1])\n",
    "    acc.append(x[1])\n",
    "    del x\n",
    "    del model\n",
    "    exit(0)\n",
    "\n",
    "# print the accuracy\n",
    "print(acc)\n",
    "a = np.array(acc)\n",
    "# print('Model:' + str(j))\n",
    "print('The averaged accuracy is:\\n')\n",
    "print(np.mean(a))\n",
    "print('The std is: \\n')\n",
    "print(np.std(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
